{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eba4f35-ed22-4de8-8124-49f100a4a593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomForTokenClassification\n",
    "from transformers import BloomForTokenClassification\n",
    "from transformers import BloomTokenizerFast\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e306dd3-50db-43f1-b7ff-67c84d318535",
   "metadata": {},
   "source": [
    "# Bloom for Causal Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8879880-3c68-450b-adb1-f7a597c2f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-1b3\")\n",
    "model = BloomForCausalLM.from_pretrained(\"bigscience/bloom-1b3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60eb2324-38f7-4a8c-afca-683c8fce0cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"It was a dark and stromy night\"\n",
    "result_length = 50\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9f3fef-8dbb-47ec-99a3-61e6a6c947df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "#loss = outputs.loss\n",
    "#logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2db68bc2-f556-4353-a40a-28db2f71cdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stromy night, and the moon was shining brightly. The\n",
      "night was full of mystery and mystery was full of light. The moon was\n",
      "shining brightly, and the stars were shining brightly, and the\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"], max_length=result_length)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be981c3e-ccc7-4cba-81f7-144d5c3e3d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a dark and stromy night, and the moon shone down upon the\n",
      "scene with a strange and mysterious radiance. The moonlight fell upon\n",
      "the face of the young man, who was lying on his back, with his hands\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       num_beams=2, \n",
    "                       no_repeat_ngram_size=2,\n",
    "                       early_stopping=True\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76b1e88d-5c47-4802-9fb1-49704155e83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_results = 3\n",
    "res = model.generate(inputs[\"input_ids\"],\n",
    "                     max_length=result_length, \n",
    "                     do_sample=True, \n",
    "                     top_k=50, \n",
    "                     top_p=0.95, \n",
    "                     num_results=num_results\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ccf1520-9e64-48fc-a266-4e2a62fdd017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Bloom large language model from Huggingface (2013) was used for the POS tags, which is based on the Stanford parser. This model uses the Stanford tagger to tag the text. As described in the following section, we experiment with two \n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1552/78620954.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1552/78620954.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "[print(tokenizer.decode(res[i]), '\\n') for i in range(num_results)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7086e7f9-bfa6-4ab2-82c4-da1859b49b21",
   "metadata": {
    "tags": []
   },
   "source": [
    "# References\n",
    "\n",
    "* https://huggingface.co/docs/transformers/model_doc/bloom\n",
    "* https://huggingface.co/blog/how-to-generate\n",
    "* https://stackoverflow.com/questions/64634027/how-to-verify-if-two-text-datasets-are-from-different-distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ac70a2-a27e-4cbd-9f6c-df419784646a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "venv",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
